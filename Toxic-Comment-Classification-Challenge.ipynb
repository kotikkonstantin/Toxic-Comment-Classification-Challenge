{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score,StratifiedKFold,train_test_split\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import f1_score,recall_score,precision_score,accuracy_score,auc,roc_curve\n",
    "\n",
    "import string\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "from itertools import cycle \n",
    "\n",
    "from __future__ import division, print_function\n",
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import scipy\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"train.csv\")\n",
    "data_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = data_train.fillna(\" \")\n",
    "data_test =data_test.fillna(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_C = data_train.comment_text\n",
    "test_C =  data_test.comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((159571,), (153164,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_C.shape, test_C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_C.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(train_C.duplicated()) #Дубликатов нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(test_C.duplicated()) #Дубликатов нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(train_C.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PREPROCESSING PART\n",
    "repl = {\n",
    "    \"&lt;3\": \" good \",\n",
    "    \":d\": \" good \",\n",
    "    \":dd\": \" good \",\n",
    "    \":p\": \" good \",\n",
    "    \"8)\": \" good \",\n",
    "    \":-)\": \" good \",\n",
    "    \":)\": \" good \",\n",
    "    \";)\": \" good \",\n",
    "    \"(-:\": \" good \",\n",
    "    \"(:\": \" good \",\n",
    "    \"yay!\": \" good \",\n",
    "    \"yay\": \" good \",\n",
    "    \"yaay\": \" good \",\n",
    "    \"yaaay\": \" good \",\n",
    "    \"yaaaay\": \" good \",\n",
    "    \"yaaaaay\": \" good \",\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" bad \",\n",
    "    \":(\": \" bad \",\n",
    "    \":s\": \" bad \",\n",
    "    \":-s\": \" bad \",\n",
    "    \"&lt;3\": \" heart \",\n",
    "    \":d\": \" smile \",\n",
    "    \":p\": \" smile \",\n",
    "    \":dd\": \" smile \",\n",
    "    \"8)\": \" smile \",\n",
    "    \":-)\": \" smile \",\n",
    "    \":)\": \" smile \",\n",
    "    \";)\": \" smile \",\n",
    "    \"(-:\": \" smile \",\n",
    "    \"(:\": \" smile \",\n",
    "    \":/\": \" worry \",\n",
    "    \":&gt;\": \" angry \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" sad \",\n",
    "    \":(\": \" sad \",\n",
    "    \":s\": \" sad \",\n",
    "    \":-s\": \" sad \",\n",
    "    r\"\\br\\b\": \"are\",\n",
    "    r\"\\bu\\b\": \"you\",\n",
    "    r\"\\bhaha\\b\": \"ha\",\n",
    "    r\"\\bhahaha\\b\": \"ha\",\n",
    "    r\"\\bdon't\\b\": \"do not\",\n",
    "    r\"\\bdoesn't\\b\": \"does not\",\n",
    "    r\"\\bdidn't\\b\": \"did not\",\n",
    "    r\"\\bhasn't\\b\": \"has not\",\n",
    "    r\"\\bhaven't\\b\": \"have not\",\n",
    "    r\"\\bhadn't\\b\": \"had not\",\n",
    "    r\"\\bwon't\\b\": \"will not\",\n",
    "    r\"\\bwouldn't\\b\": \"would not\",\n",
    "    r\"\\bcan't\\b\": \"can not\",\n",
    "    r\"\\bcannot\\b\": \"can not\",\n",
    "    r\"\\bi'm\\b\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"r\": \"are\",\n",
    "    \"u\": \"you\",\n",
    "    \"haha\": \"ha\",\n",
    "    \"hahaha\": \"ha\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"cannot\": \"can not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"i'll\" : \"i will\",\n",
    "    \"its\" : \"it is\",\n",
    "    \"it's\" : \"it is\",\n",
    "    \"'s\" : \" is\",\n",
    "    \"that's\" : \"that is\",\n",
    "    \"weren't\" : \"were not\",\n",
    "}\n",
    "\n",
    "keys = [i for i in repl.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Data preprocessing##\n",
    "\n",
    "# drop urls,punctuations,digits,special symbols in questions\n",
    "prep_c_train = []\n",
    "prep_c_test = []\n",
    "for c_train in train_C.values:\n",
    "\n",
    "    # \"ＷＨＡＴＡ  ＦＵＣＫ  ＭＡＮ\" --> \"WHATA FUCK MAN\"\n",
    "    c_train = unidecode(c_train)\n",
    "    #to lowercase\n",
    "    c_train = c_train.lower()\n",
    "\n",
    "    #drop urls\n",
    "    c_train = re.sub(r'http(s)?:\\/\\/\\S*? ', \" \", c_train)\n",
    "    #preprocessing with according to repl\n",
    "    temp = []\n",
    "    for word in c_train.split():\n",
    "        if word in keys:\n",
    "            temp += [repl[word]]\n",
    "        else:\n",
    "            temp += [word]\n",
    "\n",
    "    c_train = deepcopy(\" \".join(temp))\n",
    "    #drop digits - try dont'change\n",
    "    c_train = ''.join([i for i in c_train if not i.isdigit()])\n",
    "    #drop punctuations except apostrophes\n",
    "    p = re.compile(r\"(\\b[-']\\b)|[\\W_]\")\n",
    "        \n",
    "    prep_c_train += [p.sub(lambda m: (m.group(1) if m.group(1) else \" \"), c_train)]\n",
    "\n",
    "for c_test in test_C.values:\n",
    "    # \"ＷＨＡＴＡ  ＦＵＣＫ  ＭＡＮ\" --> \"WHATA FUCK MAN\"\n",
    "    c_test = unidecode(c_test)\n",
    "    #to lowercase\n",
    "    c_test = c_test.lower()\n",
    "\n",
    "    #drop urls\n",
    "    c_test = re.sub(r'http(s)?:\\/\\/\\S*? ', \" \", c_test) \n",
    "    #preprocessing with according to repl\n",
    "    temp = []\n",
    "    for word in c_test.split():\n",
    "        if word in keys:\n",
    "            temp += [repl[word]]\n",
    "        else:\n",
    "            temp += [word]\n",
    "\n",
    "    c_test = deepcopy(\" \".join(temp))\n",
    "    #drop digits\n",
    "    c_test = ''.join([i for i in c_test if not i.isdigit()])\n",
    "    #drop punctuations except apostrophes\n",
    "    p = re.compile(r\"(\\b[-']\\b)|[\\W_]\")\n",
    "        \n",
    "    prep_c_test += [p.sub(lambda m: (m.group(1) if m.group(1) else \" \"), c_test)]\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2veC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer() #стемминг ухудшил качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##DATA PREPARATION for Doc2vec##\n",
    "prep_c_train_split = []\n",
    "for i in range(len(prep_c_train)):\n",
    "    temp = []\n",
    "    for word in prep_c_train[i].split():\n",
    "        temp += [word]\n",
    "\n",
    "    prep_c_train_split += [temp]\n",
    "\n",
    "prep_c_train_d2v = []\n",
    "for i in range(len(prep_c_train)):\n",
    "    prep_c_train_d2v.append(\" \".join(prep_c_train_split[i]))\n",
    "\n",
    "with open(\"TRAIN_without_lemma_stem_labeled_line_sentence.txt\", \"w\") as f:\n",
    "    f.writelines(c + \"\\n\" for c in prep_c_train_d2v)\n",
    "    \n",
    "prep_c_test_split = []\n",
    "for i in range(len(prep_c_test)):\n",
    "    temp = []\n",
    "    for word in prep_c_test[i].split():\n",
    "        temp += [word]\n",
    "\n",
    "    prep_c_test_split += [temp]\n",
    "\n",
    "prep_c_test_d2v = []\n",
    "for i in range(len(prep_c_test)):\n",
    "    prep_c_test_d2v.append(\" \".join(prep_c_test_split[i]))\n",
    "\n",
    "with open(\"TEST_without_lemma_stem_labeled_line_sentence.txt\", \"w\") as f:\n",
    "    f.writelines(c + \"\\n\" for c in prep_c_test_d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prep_c_train_d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153164"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prep_c_test_d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((153164, 2), (159571, 8))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape, data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##DOC2VEC##\n",
    "\n",
    "#Modernization of LabeledLineSentence\n",
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    \"\"\"The model is better trained if in each training epoch,\n",
    "    the sequence of sentences fed to the model is randomized.\n",
    "    This is important: missing out on this steps gives you really shitty results. This is the reason for the sentences_perm method in our LabeledLineSentences class.\"\"\"\n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences\n",
    "#Also it was added opportunity to work with multiple documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n"
     ]
    }
   ],
   "source": [
    "sources = {'TRAIN_without_lemma_stem_labeled_line_sentence.txt':'TRAIN',\\\n",
    "          'TEST_without_lemma_stem_labeled_line_sentence.txt':'TEST'}\n",
    "sentences = LabeledLineSentence(sources)\n",
    "\n",
    "model = Doc2Vec(size=100, dbow_words= 1, dm=0, iter=1,  window=5, \\\n",
    "                seed=1337, min_count=1, workers=4,alpha=0.025, min_alpha=0.025)\n",
    "\n",
    "model.build_vocab(sentences.to_array())\n",
    "#model training\n",
    "for epoch in range(10):\n",
    "    print(\"epoch \"+str(epoch))\n",
    "    model.train(sentences.sentences_perm(),total_examples=model.corpus_count,epochs = 1)\n",
    "    model.alpha -= 0.002  # decrease the learning rate\n",
    "    model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('toxic.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading of Model\n",
    "model = Doc2Vec.load('toxic.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('TRAIN_115566', 0.8503616452217102), ('TRAIN_35921', 0.8322666883468628), ('TEST_46047', 0.8309272527694702), ('TRAIN_8846', 0.829414427280426), ('TEST_20784', 0.8283772468566895)]\n"
     ]
    }
   ],
   "source": [
    "# checking of Model\n",
    "print(model.docvecs.most_similar(model.infer_vector(\"what a motherfucking piece\".split()).reshape(1,-1),topn=5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_arrays = np.zeros((train_C.shape[0], SIZE))\n",
    "train_labels = np.zeros(train_C.shape[0])\n",
    "for i in range(train_C.shape[0]):\n",
    "    prefix_train = 'TRAIN_' + str(i)\n",
    "    train_arrays[i] = model.docvecs[prefix_train]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_arrays = np.zeros((test_C.shape[0], SIZE))\n",
    "test_labels = np.zeros(test_C.shape[0])\n",
    "for i in range(test_C.shape[0]):\n",
    "    prefix_test = 'TEST_' + str(i)\n",
    "    test_arrays[i] = model.docvecs[prefix_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(5,shuffle=True,random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "predictions = {'id': data_test['id']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "predictions = {'id': data_test['id']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.9668031199108047\n",
      "CV score for class severe_toxic is 0.9823977857778521\n",
      "CV score for class obscene is 0.9810258985339757\n",
      "CV score for class threat is 0.9803818370427708\n",
      "CV score for class insult is 0.9762038866419653\n",
      "CV score for class identity_hate is 0.9717900805427938\n",
      "Total CV score is 0.9764337680750271\n"
     ]
    }
   ],
   "source": [
    "for class_name in class_names:\n",
    "    train_target = data_train[class_name]\n",
    "    classifier = LogisticRegression(solver='sag')\n",
    "\n",
    "    cv_loss = np.mean(cross_val_score(classifier, train_arrays, train_target, cv=skf, scoring='roc_auc'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_loss))\n",
    "\n",
    "    classifier.fit(train_arrays, train_target)\n",
    "    predictions[class_name] = classifier.predict_proba(test_arrays)[:, 1]\n",
    "\n",
    "print('Total CV score is {}'.format(np.mean(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Total CV score is 0.976895749698627, 0.9711(PB) ---\"sag\", feature size -- 1000\n",
    "\n",
    "- Total CV score is 0.9784760833887033, 0.9723(PB) ---\"sag\", feature size -- 10000\n",
    "\n",
    "- Total CV score is 0.9779829329839521, \"sag\", feature size -- 1000, \"ＷＨＡＴＡ  ＦＵＣＫ  ＭＡＮ\" --> \"WHATA FUCK MAN\"\n",
    "\n",
    "- Total CV score is 0.9781456124579204, \"sag\", feature size -- 1000, \"ＷＨＡＴＡ  ＦＵＣＫ  ＭＡＮ\" --> \"WHATA FUCK MAN\", smiles and abbreviations transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict(predictions)\n",
    "submission.to_csv('not_tuned_log_reg_on_d2v_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konstantin/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "723"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "predictions = {'id': data_test['id']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=200, n_iter=10,\n",
       "       random_state=1, tol=0.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%time\n",
    "#svd = TruncatedSVD(n_components=200,n_iter=10,random_state=1)\n",
    "#svd.fit(train_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.9557625085446407\n",
      "CV score for class severe_toxic is 0.9806856990094197\n",
      "CV score for class obscene is 0.9673842180298122\n",
      "CV score for class threat is 0.9730923462536871\n",
      "CV score for class insult is 0.965890424679704\n",
      "CV score for class identity_hate is 0.9673515955082559\n",
      "Total CV score is 0.9683611320042532\n",
      "CPU times: user 2h 45min 16s, sys: 36.1 s, total: 2h 45min 52s\n",
      "Wall time: 21min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for class_name in class_names:\n",
    "    train_target = data_train[class_name]\n",
    "    classifier =XGBClassifier()\n",
    "\n",
    "    cv_loss = np.mean(cross_val_score(classifier, svd.transform(train_arrays), train_target, cv=skf, scoring='roc_auc'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_loss))\n",
    "\n",
    "    classifier.fit(svd.transform(train_arrays), train_target)\n",
    "    predictions[class_name] = classifier.predict_proba(svd.transform(test_arrays))[:, 1]\n",
    "\n",
    "print('Total CV score is {}'.format(np.mean(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# функция, чтобы получить NN\n",
    "def get_net(mode, ops, Xtr_shape, NUM_CLASSES=2, NEURON_NUMBER=800):\n",
    "    with tf.variable_scope('net'):\n",
    "        x = tf.placeholder(tf.float32, [None, Xtr_shape[1]])\n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        ops['x'] = x\n",
    "        ops['y'] = y\n",
    "        \n",
    "        if mode == 'implicit': # это соответствует статье \"Implicit Weight Uncertainty in Neural Networks\"\n",
    "            g1 = ops['g1']\n",
    "            g2 = ops['g2']\n",
    "            g3 = ops['g3']\n",
    "\n",
    "            \n",
    "            # layer 1\n",
    "            v1 = tf.get_variable('v1', [Xtr_shape[1], NEURON_NUMBER], tf.float32, #NEURON_NUMBER is число нейронов в слое\n",
    "                                tf.random_normal_initializer(0, 0.05))\n",
    "            \n",
    "            x = tf.matmul(x, v1)\n",
    "            \n",
    "            b1 = tf.get_variable('b1', [NEURON_NUMBER], tf.float32, tf.constant_initializer()) #смещение\n",
    "            \n",
    "            scaler = g1 / tf.sqrt(tf.reduce_sum(tf.square(v1),[0]))\n",
    "            x = tf.reshape(scaler,[1, NEURON_NUMBER])*x + tf.reshape(b1, [1, NEURON_NUMBER])\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            # layer 2\n",
    "            v2 = tf.get_variable('v2', [NEURON_NUMBER, NEURON_NUMBER], tf.float32,\n",
    "                                tf.random_normal_initializer(0, 0.05))\n",
    "            \n",
    "            x = tf.matmul(x, v2)\n",
    "            \n",
    "            b2 = tf.get_variable('b2', [NEURON_NUMBER], tf.float32, tf.constant_initializer())\n",
    "            \n",
    "            scaler = g2 / tf.sqrt(tf.reduce_sum(tf.square(v2),[0]))\n",
    "            x = tf.reshape(scaler,[1, NEURON_NUMBER])*x + tf.reshape(b2, [1, NEURON_NUMBER])\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            # layer 3\n",
    "            v3 = tf.get_variable('v3', [NEURON_NUMBER, NUM_CLASSES], tf.float32, \n",
    "                                tf.random_normal_initializer(0, 0.05))\n",
    "            \n",
    "            x = tf.matmul(x, v3)\n",
    "            \n",
    "            b3 = tf.get_variable('b3', [NUM_CLASSES], tf.float32, tf.constant_initializer()) \n",
    "            \n",
    "            scaler = g3 / tf.sqrt(tf.reduce_sum(tf.square(v3),[0]))\n",
    "            x = tf.reshape(scaler,[1, NUM_CLASSES])*x + tf.reshape(b3, [1, NUM_CLASSES])\n",
    "\n",
    "        ops['logits'] = x\n",
    "        \n",
    "        return ops\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hypernetwork, чтобы генерить веса (генеративная модель)\n",
    "def get_h_net(units=[64, 256], num_noise=29,NUM_CLASSES=2,NEURON_NUMBER=800):\n",
    "    with tf.variable_scope('h_net'):\n",
    "        # auxiliary conditioning\n",
    "        w1_c = tf.constant([1., 0., 0.])\n",
    "        w2_c = tf.constant([0., 1., 0.])\n",
    "        w3_c = tf.constant([0., 0., 1.])\n",
    "\n",
    "        #auxiliary noise\n",
    "        noise = tf.random_normal((num_noise, ))\n",
    "\n",
    "        w1_z = tf.reshape(tf.concat([w1_c, noise], 0), (1, num_noise + 3))\n",
    "        w2_z = tf.reshape(tf.concat([w2_c, noise], 0), (1, num_noise + 3))\n",
    "        w3_z = tf.reshape(tf.concat([w3_c, noise], 0), (1, num_noise + 3))\n",
    "        \n",
    "        w_z = tf.concat([w1_z, w2_z, w3_z], 0)\n",
    "        \n",
    "        z = w_z\n",
    "        \n",
    "        for unit in units:\n",
    "            z = tf.layers.dense(inputs=z, units=unit)\n",
    "            z = tf.nn.elu(z)\n",
    "        \n",
    "        z = tf.layers.dense(inputs=w_z, units=NEURON_NUMBER)\n",
    "\n",
    "        w1 = z[0, :]\n",
    "        w2 = z[1, :]\n",
    "\n",
    "        w3 = z[2, :NUM_CLASSES]\n",
    "        \n",
    "        return [w1, w2, w3, tf.reshape(tf.concat([w1, w2, w3], 0), (2*NEURON_NUMBER+NUM_CLASSES, 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#дискриминативная модель\n",
    "def get_d_net(gens, units=[20, 20],NUM_CLASSES=2,NEURON_NUMBER=800):\n",
    "    with tf.variable_scope('d_net'):\n",
    "        \n",
    "        ds = tf.contrib.distributions\n",
    "        mix = 0.7\n",
    "        bimix_gauss = ds.Mixture(\n",
    "          cat=ds.Categorical(probs=[mix, 1.-mix]),\n",
    "          components=[\n",
    "            ds.Normal(loc=0., scale=0.01),\n",
    "            ds.Normal(loc=0., scale=5.),\n",
    "        ])\n",
    "        \n",
    "        noise = bimix_gauss.sample((NEURON_NUMBER + NEURON_NUMBER + NUM_CLASSES, 1))\n",
    "        \n",
    "        all_t = tf.concat((gens, noise), 0)\n",
    "        \n",
    "        d = all_t\n",
    "        \n",
    "        for unit in units:\n",
    "            d = tf.layers.dense(inputs=d, units=unit )\n",
    "            d = tf.nn.relu(d)\n",
    "        \n",
    "        d = tf.layers.dense(inputs=d, units=1)\n",
    "        \n",
    "        return d[:NEURON_NUMBER + NEURON_NUMBER + NUM_CLASSES], d[NEURON_NUMBER + NEURON_NUMBER +NUM_CLASSES:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# адаптированная под специфику данной задачи метод tensorflow -- next_batch\n",
    "def next_batch(data:\"np array\", labels:\"np array\", batch_size:int, shuffle=False):\n",
    "    \n",
    "    #если наблюдения независимы, что в нашем случае НЕ ТАК\n",
    "    if shuffle:\n",
    "        # Shuffle data\n",
    "        shuffle_indices = np.random.permutation(np.arange(len(labels)))\n",
    "        data = data[shuffle_indices]\n",
    "        labels = labels[shuffle_indices]\n",
    "\n",
    "    for batch_i in range(0, len(data)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        data_batch = data[start_i:start_i + batch_size]\n",
    "        labels_batch = labels[start_i:start_i + batch_size]\n",
    "\n",
    "        yield np.array(data_batch), np.array(labels_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_computations(X,y,Xtest,n_epochs=200):\n",
    "    #Bayes NN\n",
    "    num_noise = 125\n",
    "\n",
    "    layers = [64, 256]\n",
    "\n",
    "    mode = 'implicit'\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    ops = {}\n",
    "\n",
    "    if mode == 'implicit':\n",
    "        w1, w2, w3, gens = get_h_net(num_noise=num_noise, units=layers)\n",
    "\n",
    "        g_d, n_d = get_d_net(gens)\n",
    "\n",
    "        ops = {'g1': w1, 'g2': w2, 'g3': w3}\n",
    "\n",
    "        d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'd_net')\n",
    "        g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'h_net')\n",
    "\n",
    "    # get network ops\n",
    "    ops = get_net(mode, ops, Xtr_shape=np.array(X.shape).astype(np.int32))\n",
    "\n",
    "    net_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'net')\n",
    "\n",
    "    # оптимизируемая функция потерь -- log-loss\n",
    "    ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=ops['logits'], labels=ops['y']))\n",
    "\n",
    "    # метод оптимизации\n",
    "    opt = tf.train.AdamOptimizer(0.001,epsilon=1e-5)\n",
    "\n",
    "    if mode == 'implicit':\n",
    "        loss_d = (- tf.reduce_mean(tf.log(1 - tf.nn.sigmoid(n_d) + 1e-8, name='log_n_d'))\n",
    "              - tf.reduce_mean(tf.log(tf.nn.sigmoid(g_d) + 1e-8, name='log_g_d')))\n",
    "\n",
    "        gvs = opt.compute_gradients(loss_d, var_list=d_vars)\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gvs if grad is not None]\n",
    "        d_optimiser = opt.apply_gradients(capped_gvs)\n",
    "\n",
    "        g_logits_m = tf.reduce_mean(g_d)\n",
    "\n",
    "        loss_g = g_logits_m + len(y) / float(batch_size) * ce\n",
    "\n",
    "        g_optimiser = opt.minimize(loss_g, var_list=g_vars+net_vars)\n",
    "\n",
    "        d_n_acc = tf.reduce_mean(tf.cast(tf.nn.sigmoid(n_d) < 0.5, tf.float32))\n",
    "        d_g_acc = tf.reduce_mean(tf.cast(tf.nn.sigmoid(g_d) >= 0.5, tf.float32))\n",
    "\n",
    "\n",
    "    # Функция, хранящая предсказания \n",
    "    pred = tf.argmax(ops['logits'], -1, output_type=tf.int32)\n",
    "\n",
    "    # Тензор, чтобы вычислить точность предсказаний\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(pred, ops['y']), tf.float32))\n",
    "\n",
    "    probs = tf.nn.softmax(ops['logits'])\n",
    "\n",
    "    # для инициализации переменных\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    numerics = tf.add_check_numerics_ops()\n",
    "\n",
    "    # генератор для циклического прохода по батчам\n",
    "    generator = cycle(next_batch(data=X,labels=y,batch_size=batch_size,shuffle=True))\n",
    "\n",
    "    s = tf.Session()\n",
    "\n",
    "    # Инициализация весов\n",
    "    s.run(init)\n",
    "\n",
    "    print(\"Xtrain shape: {}\".format(X.shape))\n",
    "    print(\"GPU: {}\".format(tf.test.is_gpu_available()))\n",
    "\n",
    "    # Запуск дискриминатора, чтобы давать лучшие значения градиентов\n",
    "    if mode == 'implicit':\n",
    "        for _ in range(300):\n",
    "            s.run(d_optimiser)\n",
    "\n",
    "    with trange(n_epochs * 400) as pbar: # проведём ~ n эпох (первый аргумент)\n",
    "        for i in pbar:\n",
    "            # получаем батч \n",
    "            b = batch_xs, batch_ys = next(generator) \n",
    "        \n",
    "            if mode == 'implicit':\n",
    "                #Запуск дискриминатора, чтобы давать лучшие значения градиентов\n",
    "                for _ in range(20):\n",
    "                    s.run(d_optimiser)\n",
    "\n",
    "                np_acc, d_loss, g_loss, l_loss, np_d, np_g, _, _ = s.run([acc, loss_d, g_logits_m, ce,\n",
    "                                                                  d_n_acc, d_g_acc, g_optimiser, numerics],\n",
    "                                                                 feed_dict={ops['x']: batch_xs,\n",
    "                                                                            ops['y']: batch_ys})\n",
    "\n",
    "                pbar.set_postfix(acc=np_acc, d_loss=d_loss, g_loss=g_loss,\n",
    "                             l_loss=l_loss, d_n_acc=np_d, d_g_acc=np_g)\n",
    "            elif mode == 'bbb':\n",
    "                np_acc, l_loss, kl_loss, _ = s.run([acc, ce, ops['kl_loss'], optimiser],\n",
    "                                      feed_dict={ops['x']: batch_xs, ops['y']: batch_ys})\n",
    "                pbar.set_postfix(acc=np_acc, ce=l_loss, kl_loss=kl_loss)\n",
    "            else:\n",
    "                np_acc, l_loss, _ = s.run([acc, ce, optimiser],\n",
    "                                      feed_dict={ops['x']: batch_xs, ops['y']: batch_ys})\n",
    "\n",
    "                pbar.set_postfix(acc=np_acc, ce=l_loss)\n",
    "                \n",
    "    NUM_CLASSES = 2\n",
    "            \n",
    "    bnn_probs = np.zeros((Xtest.shape[0], NUM_CLASSES))\n",
    "    mc_steps = 300\n",
    "    for _ in trange(mc_steps):\n",
    "        bnn_probs += s.run(probs, feed_dict={ops['x']: Xtest})\n",
    "\n",
    "    bnn_probs /= mc_steps\n",
    "    bnn_preds = np.argmax(bnn_probs, -1)\n",
    "    \n",
    "    return bnn_probs,bnn_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "predictions = {'id': data_test['id']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain shape: (119678, 100)\n",
      "GPU: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80000/80000 [52:07<00:00, 25.58it/s, acc=0.992, d_g_acc=0.979, d_loss=0.664, d_n_acc=0.758, g_loss=1.29, l_loss=0.0111] \n",
      "100%|██████████| 300/300 [00:10<00:00, 28.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold-Out score for class toxic is 0.9543874007257592\n",
      "Xtrain shape: (119678, 100)\n",
      "GPU: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 12760/80000 [08:20<47:27, 23.61it/s, acc=1, d_g_acc=0.96, d_loss=0.484, d_n_acc=0.878, g_loss=2.13, l_loss=0.0102]      \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-377c7b02839f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_arrays_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_arrays_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_target_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_target\u001b[0m \u001b[0;34m=\u001b[0m                     \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_arrays\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mbnn_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbnn_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_computations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_arrays_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_target_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_arrays_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#площадь под roc-кривой на валидационной выборке\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-dce9328bd6f1>\u001b[0m in \u001b[0;36mmake_computations\u001b[0;34m(X, y, Xtest, n_epochs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0;31m#Запуск дискриминатора, чтобы давать лучшие значения градиентов\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_optimiser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 np_acc, d_loss, g_loss, l_loss, np_d, np_g, _, _ = s.run([acc, loss_d, g_logits_m, ce,\n",
      "\u001b[0;32m/home/konstantin/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/konstantin/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/konstantin/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/konstantin/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/konstantin/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for class_name in class_names:\n",
    "    \n",
    "    train_target = data_train[class_name].values\n",
    "    \n",
    "    train_arrays_train,train_arrays_valid,train_target_tr, valid_target = \\\n",
    "                    train_test_split(train_arrays,train_target,test_size=0.25,shuffle=True,random_state=7)\n",
    "    \n",
    "    bnn_probs, bnn_preds = make_computations(X=train_arrays_train,y=train_target_tr,Xtest=train_arrays_valid,n_epochs=200)\n",
    "    \n",
    "    #площадь под roc-кривой на валидационной выборке\n",
    "    fpr,tpr,_ = roc_curve(valid_target, bnn_probs[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    losses.append(roc_auc)\n",
    "    \n",
    "    print('Hold-Out score for class {} is {}'.format(class_name,roc_auc))\n",
    "    predictions[class_name] = bnn_preds \n",
    "\n",
    "    \n",
    "print('Total CV score is {}'.format(np.mean(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идеи для дальнейшего улучшения:\n",
    "- oversampling or undersampling\n",
    "- тюнинг гиперпараметров\n",
    "- стемминг\n",
    "- фильтры слов\n",
    "- блендинг\n",
    "- стэкинг\n",
    "- см hotness кернелы и discussion\n",
    "- см канал в ods по этому соревнованию\n",
    "- увеличить размерность векторного представления документа\n",
    "- balanced classes,data augmentation\n",
    "- try to increase num of epochs for doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "predictions = {'id': data_test['id']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for class_name in class_names:\n",
    "    train_target = data_train[class_name]\n",
    "    classifier = KNeighborsClassifier()\n",
    "\n",
    "    cv_loss = np.mean(cross_val_score(classifier, train_arrays, train_target, cv=skf, scoring='roc_auc'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_loss))\n",
    "\n",
    "    classifier.fit(train_arrays, train_target)\n",
    "    predictions[class_name] = classifier.predict_proba(test_arrays)[:, 1]\n",
    "\n",
    "print('Total CV score is {}'.format(np.mean(losses))) #MEMORY ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# [CNN GLOVE300 3-OOF 4 epochs](https://www.kaggle.com/tunguz/cnn-glove300-3-oof-4-epochs/code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embeddings vectors\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'list_sentences_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5f2bd7546b4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep_c_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# + list(list_sentences_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mlist_tokenized_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_sentences_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0mlist_tokenized_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_sentences_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'padding sequences'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_sentences_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Fork of Sergei Fironov's script CNN GLOVE300 3-OOF 3 epochs\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, Concatenate, Conv1D, Activation, TimeDistributed, Flatten, RepeatVector, Permute,multiply\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, GRU, GlobalAveragePooling1D, MaxPooling1D, SpatialDropout1D, BatchNormalization\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print('loading embeddings vectors')\n",
    "def get_coefs(word,*arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(' ')) for o in open('glove.840B.300d.txt'))\n",
    "\n",
    "min_count = 10 #the minimum required word frequency in the text\n",
    "max_features = 27403 #it's from previous run with min_count=10\n",
    "maxlen = 100 #padding length\n",
    "num_folds = 3 #number of folds\n",
    "batch_size = 512 \n",
    "epochs = 4\n",
    "embed_size = 300 #embeddings dimension\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "#train = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\n",
    "#test = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\n",
    "\n",
    "#list_sentences_train = train[\"comment_text\"].fillna(\"\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = data_train[list_classes].values\n",
    "#list_sentences_test = test[\"comment_text\"].fillna(\"\").values\n",
    "\n",
    "prep_c_train = np.array(prep_c_train)\n",
    "prep_c_test = np.array(prep_c_test)\n",
    "#print('mean text len:',prep_c_train.str.count('\\S+').mean())\n",
    "#print('max text len:',prep_c_test.str.count('\\S+').max())\n",
    "\n",
    "#tokenizer = Tokenizer()\n",
    "#tokenizer.fit_on_texts(list(list_sentences_train)) #  + list(list_sentences_test)\n",
    "#num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "#print('num_words',num_words)\n",
    "#max_features = num_words\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(prep_c_train)) # + list(list_sentences_test)\n",
    "\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "print('padding sequences')\n",
    "X_train = {}\n",
    "X_test = {}\n",
    "X_train['text'] = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen, padding='post', truncating='post')\n",
    "X_test['text'] = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "print('numerical variables')\n",
    "data_train['num_words'] = data_train.comment_text.str.count('\\S+')\n",
    "data_test['num_words'] = data_test.comment_text.str.count('\\S+')\n",
    "data_train['num_comas'] = data_train.comment_text.str.count('\\.')\n",
    "data_test['num_comas'] = data_test.comment_text.str.count('\\.')\n",
    "data_train['num_bangs'] = data_train.comment_text.str.count('\\!')\n",
    "data_test['num_bangs'] = data_test.comment_text.str.count('\\!')\n",
    "data_train['num_quotas'] = data_train.comment_text.str.count('\\\"')\n",
    "data_test['num_quotas'] = data_test.comment_text.str.count('\\\"')\n",
    "data_train['avg_word'] = data_train.comment_text.str.len() / (1 + data_train.num_words)\n",
    "data_test['avg_word'] = data_test.comment_text.str.len() / (1 + data_test.num_words)\n",
    "#print('sentiment')\n",
    "#train['sentiment'] = train.comment_text.apply(lambda s : sia.polarity_scores(s)['compound'])\n",
    "#test['sentiment'] = test.comment_text.apply(lambda s : sia.polarity_scores(s)['compound'])\n",
    "scaler = MinMaxScaler()\n",
    "X_train['num_vars'] = scaler.fit_transform(data_train[['num_words','num_comas','num_bangs','num_quotas','avg_word']])\n",
    "X_test['num_vars'] = scaler.transform(data_test[['num_words','num_comas','num_bangs','num_quotas','avg_word']])\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "print('create embedding matrix')\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "def get_model_cnn(X_train):\n",
    "    global embed_size\n",
    "    inp = Input(shape=(maxlen, ), name=\"text\")\n",
    "    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    z = GlobalMaxPool1D()(x)\n",
    "    x = GlobalMaxPool1D()(Conv1D(embed_size, 4, activation=\"relu\")(x))\n",
    "    x = Concatenate()([x,z,num_vars])\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=[inp,num_vars], outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model        \n",
    "\n",
    "print('start modeling')\n",
    "scores = []\n",
    "predict = np.zeros((test.shape[0],6))\n",
    "oof_predict = np.zeros((train.shape[0],6))\n",
    "\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=239)\n",
    "for train_index, test_index in kf.split(X_train['num_vars']):\n",
    "    kfold_X_train = {}\n",
    "    kfold_X_valid = {}\n",
    "    y_train,y_test = y[train_index], y[test_index]\n",
    "    for c in ['text','num_vars']:\n",
    "        kfold_X_train[c] = X_train[c][train_index]\n",
    "        kfold_X_valid[c] = X_train[c][test_index]\n",
    "\n",
    "    model = get_model_cnn(X_train)\n",
    "    model.fit(kfold_X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "    predict += model.predict(X_test, batch_size=1000) / num_folds\n",
    "    oof_predict[test_index] = model.predict(kfold_X_valid, batch_size=1000)\n",
    "    cv_score = roc_auc_score(y_test, oof_predict[test_index])\n",
    "    scores.append(cv_score)\n",
    "    print('score: ',cv_score)\n",
    "\n",
    "print('Total CV score is {}'.format(np.mean(scores)))    \n",
    "\n",
    "\n",
    "sample_submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "oof = pd.DataFrame.from_dict({'id': train['id']})\n",
    "for c in list_classes:\n",
    "    oof[c] = np.zeros(len(train))\n",
    "    sample_submission[c] = np.zeros(len(test))\n",
    "    \n",
    "sample_submission[list_classes] = predict\n",
    "sample_submission.to_csv('submit_cnn_avg_' + str(num_folds) + '_folds.csv', index=False)\n",
    "\n",
    "oof[list_classes] = oof_predict\n",
    "oof.to_csv('cnn_'+str(num_folds)+'_oof.csv', index=False)\n",
    "\n",
    "#Memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-42264d28f181>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep_c_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\S+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.array(prep_c_train).str.count('\\S+').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU like [here](https://github.com/PavelOstyakov/toxic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bojan = pd.read_csv('logistic_regression_with_words_and_char_n_grams.csv')\n",
    "gru_ostyakov_with_my_preprocessing = pd.read_csv(\"submit_with_kost_prep.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myd2v = pd.read_csv('not_tuned_log_reg_on_d2v_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/jhoward/minimal-lstm-nb-svm-baseline-ensemble\n",
    "pub2 = pd.read_csv(\"Minimal_LSTM + NB-SVM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gru_ostyakov_with_my_preprocessing = gru_ostyakov_with_my_preprocessing[[\"id\",\"identity_hate\",\"insult\",\"obscene\",\"severe_toxic\",\"threat\",\"toxic\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probas=pd.DataFrame(0.85*gru_ostyakov_with_my_preprocessing.values[:,1:]+0.15*myd2v.values[:,1:],columns=[\"identity_hate\",\"insult\",\"obscene\",\"severe_toxic\",\"threat\",\"toxic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probas=pd.DataFrame(0.85*gru_ostyakov_with_my_preprocessing.values[:,1:]+0.15*myd2v.values[:,1:],columns=[\"identity_hate\",\"insult\",\"obscene\",\"severe_toxic\",\"threat\",\"toxic\"])\n",
    "pd.concat([myd2v.id,probas],1).to_csv(\"w_mean_myd2v_gru_ostyakov_with_my_preprocessing.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_of_gru_ostyakov_with_my_preprocessing_and_myd2v = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ost, my in zip(gru_ostyakov_with_my_preprocessing.values[:,1:],myd2v.values[:,1:]):\n",
    "    MAX_of_gru_ostyakov_with_my_preprocessing_and_myd2v.append(np.max(np.vstack((ost,my)),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_of_gru_ostyakov_with_my_preprocessing_and_myd2v = np.array(MAX_of_gru_ostyakov_with_my_preprocessing_and_myd2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probas=pd.DataFrame(MAX_of_gru_ostyakov_with_my_preprocessing_and_myd2v,columns=[\"identity_hate\",\"insult\",\"obscene\",\"severe_toxic\",\"threat\",\"toxic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.concat([myd2v.id,probas],1).to_csv(\"max_of_myd2v_gru_ostyakov_with_my_preprocessing.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [url](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/kernels?sortBy=score-desc&group=everyone&pageSize=20&competitionId=8076)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hight_of_blend = pd.read_csv(\"hight_of_blend_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas=pd.DataFrame(0.55*hight_of_blend.values[:,1:]+0.45*gru_ostyakov_with_my_preprocessing.values[:,1:],columns=[\"identity_hate\",\"insult\",\"obscene\",\"severe_toxic\",\"threat\",\"toxic\"])\n",
    "pd.concat([myd2v.id,probas],1).to_csv(\"w_mean_hight_of_blend_ostyakov_gru_with_preprocessing.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " best = pd.read_csv(\"w_mean_hight_of_blend_myd2v_ostyakov_lstm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probas=pd.DataFrame(0.5*best.values[:,1:]+0.5*gru_ostyakov_with_my_preprocessing.values[:,1:],columns=[\"identity_hate\",\"insult\",\"obscene\",\"severe_toxic\",\"threat\",\"toxic\"])\n",
    "pd.concat([myd2v.id,probas],1).to_csv(\"w_mean_best_gru_with_preprocessing.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
